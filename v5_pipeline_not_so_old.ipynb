{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7a0ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DAW Pipeline: Velo/Fuss + MeteoSwiss\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# ---------------- Configuration ----------------\n",
    "PROJECT_PATH = Path(\"/Users/FHNW/Documents/daw/project\")  # adapt if needed\n",
    "\n",
    "try:\n",
    "    os.chdir(PROJECT_PATH)\n",
    "    print(f\"Working directory: {os.getcwd()}\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"Path '{PROJECT_PATH}' not found!\")\n",
    "\n",
    "VELO_CSV = \"converted_Velo_Fuss_Count.csv\"\n",
    "METEO_CSV = \"ogd-smn_bas_h_historical_2020-2029.csv\"\n",
    "METEO_META = \"ogd-smn_meta_parameters.csv\"\n",
    "\n",
    "YEARS = [2024]      # year(s) to analyze\n",
    "CHUNKSIZE = 500_000 # rows per chunk when streaming Velo data\n",
    "\n",
    "print(\"\\nAnalyzing years:\", YEARS, \"with chunk size:\", CHUNKSIZE, \"\\n\")\n",
    "\n",
    "\n",
    "# ---------------- MeteoSwiss: load + aggregate ----------------\n",
    "def load_meteo_data(meteo_csv, meta_csv):\n",
    "    \"\"\"Load MeteoSwiss data and rename columns using metadata.\"\"\"\n",
    "    print(\"--> Loading MeteoSwiss data and metadata...\\n\")\n",
    "    data = pd.read_csv(meteo_csv, sep=\";\", encoding=\"latin1\")\n",
    "    meta = pd.read_csv(meta_csv, sep=\";\", encoding=\"latin1\")\n",
    "\n",
    "    print(\"Original MeteoSwiss data (head):\")\n",
    "    display(data.head(3))\n",
    "    print(\"Metadata (head):\")\n",
    "    display(meta.head(3))\n",
    "\n",
    "    rename_dict = dict(\n",
    "        zip(meta[\"parameter_shortname\"], meta[\"parameter_description_en\"])\n",
    "    )\n",
    "    data = data.rename(columns=rename_dict)\n",
    "\n",
    "    data[\"reference_timestamp\"] = pd.to_datetime(\n",
    "        data[\"reference_timestamp\"],\n",
    "        format=\"%d.%m.%Y %H:%M\",\n",
    "        dayfirst=True,\n",
    "        errors=\"coerce\",\n",
    "    )\n",
    "\n",
    "    print(\"\\nMeteoSwiss data with parsed timestamps and readable column names:\")\n",
    "    display(data.head(3))\n",
    "    return data\n",
    "\n",
    "\n",
    "def aggregate_daily_meteo(df, years):\n",
    "    \"\"\"Aggregate hourly MeteoSwiss data to daily temp/precip/wind.\"\"\"\n",
    "    print(\"\\nAggregating MeteoSwiss data to daily level...\")\n",
    "    df = df[df[\"reference_timestamp\"].dt.year.isin(years)].copy()\n",
    "\n",
    "    daily = (\n",
    "        df.groupby(df[\"reference_timestamp\"].dt.floor(\"D\"))\n",
    "        .agg(\n",
    "            temp_mean_C=(\"Air temperature 2 m above ground; hourly mean\", \"mean\"),\n",
    "            precip_mm=(\"Precipitation; hourly total\", \"sum\"),\n",
    "            wind_mean_ms=(\"Wind speed scalar; hourly mean in m/s\", \"mean\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "        .rename(columns={\"reference_timestamp\": \"date\"})\n",
    "    )\n",
    "\n",
    "    print(\"Daily MeteoSwiss data (head):\")\n",
    "    display(daily.head(3))\n",
    "    return daily\n",
    "\n",
    "\n",
    "# ---------------- Velo/Fuss: streaming + aggregate ----------------\n",
    "def aggregate_velo_streaming(velo_csv, years, chunksize):\n",
    "    \"\"\"\n",
    "    Stream Velo/Fuss CSV and aggregate to daily totals per site.\n",
    "\n",
    "    Returns columns: ['SiteCode', 'SiteName', 'date', 'daily_total'].\n",
    "    \"\"\"\n",
    "    print(\"\\n--> Loading Velo/Fuss data in streaming mode...\")\n",
    "    print(\"Aggregating daily totals while streaming...\\n\")\n",
    "\n",
    "    usecols = [\"Date\", \"SiteCode\", \"SiteName\", \"Total\"]\n",
    "    daily_agg = []\n",
    "    chunk_count = 0\n",
    "\n",
    "    for chunk in pd.read_csv(\n",
    "        velo_csv,\n",
    "        sep=\";\",\n",
    "        encoding=\"utf-8\",  # Basel portal is UTF-8; fixes 'Dreirosenbrücke'\n",
    "        usecols=usecols,\n",
    "        chunksize=chunksize,\n",
    "        low_memory=False,\n",
    "    ):\n",
    "        chunk_count += 1\n",
    "\n",
    "        # Parse dates and keep only valid ones in the selected years\n",
    "        chunk[\"Date\"] = pd.to_datetime(\n",
    "            chunk[\"Date\"], errors=\"coerce\", dayfirst=True\n",
    "        )\n",
    "        chunk = chunk.dropna(subset=[\"Date\"])\n",
    "        chunk = chunk[chunk[\"Date\"].dt.year.isin(years)]\n",
    "        if chunk.empty:\n",
    "            continue\n",
    "\n",
    "        # Ensure numeric counts and drop negatives\n",
    "        chunk[\"Total\"] = pd.to_numeric(chunk[\"Total\"], errors=\"coerce\")\n",
    "        chunk = chunk.dropna(subset=[\"Total\"])\n",
    "        chunk = chunk[chunk[\"Total\"] >= 0]\n",
    "\n",
    "        # Create daily date column and aggregate per site + date\n",
    "        chunk[\"date\"] = chunk[\"Date\"].dt.floor(\"D\")\n",
    "        daily = (\n",
    "            chunk.groupby([\"SiteCode\", \"SiteName\", \"date\"], as_index=False)[\"Total\"]\n",
    "            .sum()\n",
    "        )\n",
    "        daily_agg.append(daily)\n",
    "\n",
    "    if not daily_agg:\n",
    "        raise ValueError(\"No Velo/Fuss data found for requested years.\")\n",
    "\n",
    "    result = pd.concat(daily_agg, ignore_index=True)\n",
    "    result = result.rename(columns={\"Total\": \"daily_total\"})\n",
    "\n",
    "    print(f\"Finished processing {chunk_count} chunks.\")\n",
    "    print(\"Daily Velo/Fuss data (head):\")\n",
    "    display(result.head(3))\n",
    "    return result\n",
    "\n",
    "\n",
    "# ---------------- Helper: missing days per year ----------------\n",
    "def check_missing_days(df, col, years):\n",
    "    \"\"\"Count missing calendar days per year based on a datetime column.\"\"\"\n",
    "    out = {}\n",
    "    for y in years:\n",
    "        expected = pd.date_range(f\"{y}-01-01\", f\"{y}-12-31\", freq=\"D\")\n",
    "        present = df.loc[df[col].dt.year == y, col].drop_duplicates()\n",
    "        missing = expected.difference(present)\n",
    "        out[y] = len(missing)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---------------- Merge + simple feature engineering ----------------\n",
    "def merge_datasets(daily_velo, daily_meteo):\n",
    "    \"\"\"\n",
    "    Merge Velo/Fuss and Meteo data on 'date' and add weekday/weekend features.\n",
    "    \"\"\"\n",
    "    print(\"\\nMerging Velo/Fuss and Meteo datasets...\")\n",
    "    daily_velo[\"date\"] = pd.to_datetime(daily_velo[\"date\"])\n",
    "    daily_meteo[\"date\"] = pd.to_datetime(daily_meteo[\"date\"])\n",
    "\n",
    "    merged = pd.merge(daily_velo, daily_meteo, on=\"date\", how=\"left\")\n",
    "\n",
    "    merged[\"weekday\"] = merged[\"date\"].dt.day_name()\n",
    "    merged[\"is_weekend\"] = merged[\"date\"].dt.dayofweek >= 5\n",
    "\n",
    "    print(f\"Merged dataset shape: {merged.shape}\")\n",
    "    print(\"Merged dataset (head):\")\n",
    "    display(merged.head(3))\n",
    "    return merged\n",
    "\n",
    "\n",
    "# ---------------- Plots (time series + scatter) ----------------\n",
    "def plot_weekly_bar(merged_df, years):\n",
    "    \"\"\"Weekly total counts across all sites.\"\"\"\n",
    "    print(\"\\nGenerating weekly bar charts...\")\n",
    "    for year in years:\n",
    "        df_year = merged_df[merged_df[\"date\"].dt.year == year]\n",
    "        weekly = (\n",
    "            df_year.groupby(df_year[\"date\"].dt.to_period(\"W\"))[\"daily_total\"]\n",
    "            .sum()\n",
    "            .reset_index()\n",
    "        )\n",
    "        weekly[\"week_start\"] = weekly[\"date\"].dt.start_time\n",
    "        tick_idx = np.arange(0, len(weekly), 4)\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        sns.barplot(data=weekly, x=\"week_start\", y=\"daily_total\")\n",
    "        plt.title(f\"Weekly total Velo/Fuss counts ({year})\")\n",
    "        plt.xlabel(\"Week start date\")\n",
    "        plt.ylabel(\"Total counts\")\n",
    "        plt.xticks(\n",
    "            tick_idx,\n",
    "            weekly[\"week_start\"].dt.strftime(\"%Y-%m-%d\").iloc[tick_idx],\n",
    "            rotation=45,\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_scatter_trends(merged_df, cols):\n",
    "    \"\"\"Scatterplots of daily_total vs selected meteo variables with linear trend (seaborn).\"\"\"\n",
    "    print(\"\\nGenerating scatterplots with linear trends...\")\n",
    "    for col in cols:\n",
    "        df = merged_df[[col, \"daily_total\"]].dropna()\n",
    "\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.regplot(\n",
    "            data=df,\n",
    "            x=col,\n",
    "            y=\"daily_total\",\n",
    "            scatter_kws={\"alpha\": 0.2, \"s\":12},\n",
    "            line_kws={\"color\": \"red\", \"linewidth\": 1},\n",
    "        )\n",
    "        plt.title(f\"Daily total Velo/Fuss vs {col}\")\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel(\"Daily total\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# ---------------- Some extra analysis ----------------\n",
    "def analyze_relationships(merged_df):\n",
    "    \"\"\"\n",
    "    Print and visualize simple group statistics to answer high-level research questions.\n",
    "\n",
    "    - Temperature bins vs mean counts\n",
    "    - Precipitation categories vs mean counts\n",
    "    - Weekday vs weekend differences\n",
    "    \"\"\"\n",
    "    df = merged_df.copy()\n",
    "\n",
    "    # --- Temperature bins ---\n",
    "    temp_bins = [-20, 0, 10, 15, 20, 25, 30, 45]\n",
    "    temp_labels = [\"<0\", \"0–10\", \"10–15\", \"15–20\", \"20–25\", \"25–30\", \">30\"]\n",
    "    df[\"temp_bin\"] = pd.cut(\n",
    "        df[\"temp_mean_C\"],\n",
    "        bins=temp_bins,\n",
    "        labels=temp_labels,\n",
    "        include_lowest=True,\n",
    "    )\n",
    "\n",
    "    temp_means = df.groupby(\"temp_bin\", observed=True)[\"daily_total\"].mean().round(1)\n",
    "    print(\"\\nMean daily_total by temperature bin:\")\n",
    "    print(temp_means)\n",
    "\n",
    "    temp_means_df = temp_means.reset_index(name=\"mean_daily_total\")\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.barplot(data=temp_means_df, x=\"temp_bin\", y=\"mean_daily_total\")\n",
    "    plt.ylabel(\"Mean daily total\")\n",
    "    plt.xlabel(\"Daily mean temperature (°C bin)\")\n",
    "    plt.title(\"Mean daily counts by temperature bin\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Precipitation categories ---\n",
    "    precip_bins = [-0.01, 0.1, 5, 1000]\n",
    "    precip_labels = [\"Dry\", \"Light rain\", \"Rainy\"]\n",
    "    df[\"precip_cat\"] = pd.cut(\n",
    "        df[\"precip_mm\"],\n",
    "        bins=precip_bins,\n",
    "        labels=precip_labels,\n",
    "        include_lowest=True,\n",
    "    )\n",
    "\n",
    "    precip_means = df.groupby(\"precip_cat\", observed=True)[\"daily_total\"].mean().round(1)\n",
    "    print(\"\\nMean daily_total by precipitation category:\")\n",
    "    print(precip_means)\n",
    "\n",
    "    precip_means_df = precip_means.reset_index(name=\"mean_daily_total\")\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.barplot(data=precip_means_df, x=\"precip_cat\", y=\"mean_daily_total\")\n",
    "    plt.ylabel(\"Mean daily total\")\n",
    "    plt.xlabel(\"Precipitation category\")\n",
    "    plt.title(\"Mean daily counts by precipitation category\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Weekday pattern ---\n",
    "    weekday_means = df.groupby(\"weekday\")[\"daily_total\"].mean().round(1)\n",
    "    print(\"\\nMean daily_total by weekday:\")\n",
    "    print(weekday_means)\n",
    "\n",
    "    order = [\n",
    "        \"Monday\",\n",
    "        \"Tuesday\",\n",
    "        \"Wednesday\",\n",
    "        \"Thursday\",\n",
    "        \"Friday\",\n",
    "        \"Saturday\",\n",
    "        \"Sunday\",\n",
    "    ]\n",
    "    weekday_means_df = weekday_means.loc[order].reset_index(name=\"mean_daily_total\")\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.barplot(\n",
    "        data=weekday_means_df,\n",
    "        x=\"weekday\",\n",
    "        y=\"mean_daily_total\",\n",
    "    )\n",
    "    plt.ylabel(\"Mean daily total\")\n",
    "    plt.xlabel(\"Weekday\")\n",
    "    plt.title(\"Mean daily counts by weekday\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Weekday vs weekend ---\n",
    "    weekend_means = df.groupby(\"is_weekend\")[\"daily_total\"].mean().round(1)\n",
    "    print(\"\\nMean daily_total: weekday vs weekend:\")\n",
    "    print(weekend_means)\n",
    "\n",
    "    weekend_means_df = weekend_means.reset_index(name=\"mean_daily_total\")\n",
    "    weekend_means_df[\"label\"] = weekend_means_df[\"is_weekend\"].map(\n",
    "        {False: \"Weekday\", True: \"Weekend\"}\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.barplot(\n",
    "        data=weekend_means_df,\n",
    "        x=\"label\",\n",
    "        y=\"mean_daily_total\",\n",
    "    )\n",
    "    plt.ylabel(\"Mean daily total\")\n",
    "    plt.xlabel(\"\")\n",
    "    plt.title(\"Mean daily counts: weekday vs weekend\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ---------------- Run full pipeline ----------------\n",
    "def run_pipeline():\n",
    "    \"\"\"Execute the full DAW pipeline and save merged dataset.\"\"\"\n",
    "    meteo_raw = load_meteo_data(METEO_CSV, METEO_META)\n",
    "    daily_meteo = aggregate_daily_meteo(meteo_raw, YEARS)\n",
    "\n",
    "    daily_velo = aggregate_velo_streaming(VELO_CSV, YEARS, CHUNKSIZE)\n",
    "\n",
    "    print(\"\\nChecking missing days in both datasets:\")\n",
    "    print(\"MeteoSwiss:\", check_missing_days(daily_meteo, \"date\", YEARS))\n",
    "    print(\"Velo/Fuss:\", check_missing_days(daily_velo, \"date\", YEARS))\n",
    "\n",
    "    merged = merge_datasets(daily_velo, daily_meteo)\n",
    "\n",
    "    plot_weekly_bar(merged, YEARS)\n",
    "    plot_scatter_trends(merged, [\"temp_mean_C\", \"precip_mm\", \"wind_mean_ms\"])\n",
    "    analyze_relationships(merged)\n",
    "\n",
    "    merged.to_csv(\"merged_dataset.csv\", index=False)\n",
    "    print(\"\\nMerged dataset saved as 'merged_dataset.csv'.\")\n",
    "    return merged\n",
    "\n",
    "\n",
    "# Run pipeline\n",
    "merged_df = run_pipeline()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
